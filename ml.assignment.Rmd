---
title: "Machine Learning Assignment"
author: "Mihhail Fjodorov"
date: "Sunday, July 26, 2015"
output: html_document
---

***Summary***
The purpose of this report is to describe the procedure of creating a Machine Learning Algorithm
for prediction. I explored several possibilites as Classification Trees and Random Forest.
I split the training data 50/50 to get the out of Sample Classification Error. I was able to achieve
Accuracy of around 89 % with Trees and Accuracy of around 99 % with Random Forests.
I used the Random Forests Algorithm on the Validation Data Set and got a result of 20/20.

***Assignment and Data***
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

***Getting the Data***

```{r}
# Load the required Packages
library(caret)
library(rpart)
library(ggplot2)
library(randomForest)
library(rattle)

# url for the training data set

url <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
# url for the vaildation data set

url2 <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
# download both data sets

download.file(url, destfile = "./training.csv", quiet = F)
download.file(url2, destfile = "./validation.csv", quiet = F)
# read both data sets in to R

training <- read.csv("./training.csv", header = T,na.strings = c("","NA","#DIV/0!"))
validation <- read.csv("./validation.csv", header = T,na.strings = c("","NA","#DIV/0!"))
```

***Variable Transformations and Data Cleaning***

In order to identify the best predictors I have excluded all of the features which could be considered to have low variance. Several features had a lot of NA values so I dedided to exclude those features as well.

```{r}
zeroVar <- nearZeroVar(training,saveMetrics = T)
training <- training[,!zeroVar[,4]]
validation <- validation[,!zeroVar[,4]]
nas <- colSums(sapply(training, is.na))
training <- training[,nas == 0]
validation <- validation[,nas == 0]
```


Furthermore, I have excluded the X variable from the data set since it is just an index and has no predictive value. Additionaly, I decided to leave the timestamp variables and the username variables in because without them the accuracy fell atleast 40%. I am a bit concerned about leaving the username becuase the algorithm might
expreince issue with scaling on the new data set.
Finally I have converted all of the data to Numeric since Random Forests prefers all of the features to be of
the same type.

```{r}
training_2 <- training[,-c(1,6)]
validation_2 <- validation[,-c(1,6)]
for(i in 1:(dim(training_2)[2]-1)){training_2[,i] <- as.numeric(training_2[,i])}
for(i in 1:(dim(validation_2)[2]-1)){validation_2[,i] <- as.numeric(validation_2[,i])}
```

I split the Training Data into Training and Testing and decided to use the offical Testing data as a Validation
Set. I have used 50/50 split becuase I dont have enough RAM to work with a larger Data Set.

```{r}
inTrain <- createDataPartition(training_2$classe, p = 0.5, list = F) 
train <- training_2[inTrain,]
test <- training_2[-inTrain,]
```

***ML Alorithms***

Since the replationship between the outcome and predictors does not appear to be linear
I decided to use the Classification Trees.

```{r}
# use classification trees for the initial model
modFit_tree <- rpart(classe ~ ., data=train, method="class")
prediction_tree <- predict(modFit_tree, test, type = "class")
confusionMatrix(prediction_tree, test$classe)
```

Creating a Tree Plot to visualize the spilt
```{r}
fancyRpartPlot(modFit_tree)
```

Classification Trees worked quite well, but I think we can do even better with Ranom Forests.


```{r, echo = FALSE}
# remove some data to free up memory
rm(training,training_2)
```

```{r}
## random forests
modFit_forest <- randomForest(classe ~. , data=train)
prediction_forest <- predict(modFit_forest, test, type = "class")
confusionMatrix(prediction_forest, test$classe)
```

Random Forests turn out to be a much better predictor and we will use it on the validation data set

```{r}
# predict the values on the validation data set
answers <- predict(modFit_forest, validation_2, type = "class")

# submission code
pml_write_files = function(x){
        n = length(x)
        for(i in 1:n){
                filename = paste0("problem_id_",i,".txt")
                write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
        }
}
pml_write_files(answers)
```